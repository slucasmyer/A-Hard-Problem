{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A-Hard-Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in /Users/sullivanmyer/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (3.3.1)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sullivanmyer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sullivanmyer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sullivanmyer/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import sklearn as sklearn\n",
    "import gensim as gensim\n",
    "import keras as keras\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import time\n",
    "import re\n",
    "import twint\n",
    "import json\n",
    "import gensim.corpora as corpora\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from pprint import pprint\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in csv files and concatenate\n",
    "df1 = pd.read_csv('tweet_nlp/just_cv/twint_3_en.csv')\n",
    "df2 = pd.read_csv('tweet_nlp/just_cv/twint_4_en.csv')\n",
    "df3 = pd.read_csv('tweet_nlp/just_cv/twint_5_en.csv')\n",
    "df4 = pd.read_csv('tweet_nlp/just_cv/twint_6_en.csv')\n",
    "df5 = pd.read_csv('tweet_nlp/just_cv/twint_7_en.csv')\n",
    "df6 = pd.read_csv('tweet_nlp/just_cv/twint_8_en.csv')\n",
    "df = pd.concat([df1,df2,df3,df4,df5,df6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85716, 34)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>timezone</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>place</th>\n",
       "      <th>tweet</th>\n",
       "      <th>mentions</th>\n",
       "      <th>urls</th>\n",
       "      <th>photos</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>cashtags</th>\n",
       "      <th>link</th>\n",
       "      <th>retweet</th>\n",
       "      <th>quote_url</th>\n",
       "      <th>video</th>\n",
       "      <th>near</th>\n",
       "      <th>geo</th>\n",
       "      <th>source</th>\n",
       "      <th>user_rt_id</th>\n",
       "      <th>user_rt</th>\n",
       "      <th>retweet_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>retweet_date</th>\n",
       "      <th>translate</th>\n",
       "      <th>trans_src</th>\n",
       "      <th>trans_dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1242886541418037251</td>\n",
       "      <td>1241512518721376257</td>\n",
       "      <td>1585162217000</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>14:50:17</td>\n",
       "      <td>EDT</td>\n",
       "      <td>2758562689</td>\n",
       "      <td>adrisml777</td>\n",
       "      <td>Adrian Finn George</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I said they are better than nothing, not that ...</td>\n",
       "      <td>['mmmbiata', 'iatemuggles']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/AdrisML777/status/12428865...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'user_id': '2758562689', 'username': 'AdrisM...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1242886541271273473</td>\n",
       "      <td>1242886541271273473</td>\n",
       "      <td>1585162217000</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>14:50:17</td>\n",
       "      <td>EDT</td>\n",
       "      <td>1212444154761863168</td>\n",
       "      <td>sammendment</td>\n",
       "      <td>Second Amendment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oops: CNN Forgot To Translate Its Latest Coron...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['https://babylonbee.com/news/cnn-embarassed-a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/SAmmendment/status/1242886...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'user_id': '1212444154761863168', 'username'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1242886540839305216</td>\n",
       "      <td>1242886540839305216</td>\n",
       "      <td>1585162217000</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>14:50:17</td>\n",
       "      <td>EDT</td>\n",
       "      <td>939935680137920515</td>\n",
       "      <td>still_a_nerd</td>\n",
       "      <td>the coronavirus movie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Speeding up https://twitter.com/ryanfiredragon...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['https://twitter.com/ryanfiredragon/status/12...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/still_a_nerd/status/124288...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://twitter.com/ryanfiredragon/status/1242...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'user_id': '939935680137920515', 'username':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1242886539786350592</td>\n",
       "      <td>1242886539786350592</td>\n",
       "      <td>1585162217000</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>14:50:17</td>\n",
       "      <td>EDT</td>\n",
       "      <td>144702285</td>\n",
       "      <td>sebasaherrera</td>\n",
       "      <td>Sebastian Herrera</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Texas Gov. Greg Abbott, who routinely overrule...</td>\n",
       "      <td>['efindell', 'wsj']</td>\n",
       "      <td>['https://www.wsj.com/articles/texas-coronavir...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/SebasAHerrera/status/12428...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'user_id': '144702285', 'username': 'SebasAH...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1242886539564257292</td>\n",
       "      <td>1242834296358014977</td>\n",
       "      <td>1585162217000</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>14:50:17</td>\n",
       "      <td>EDT</td>\n",
       "      <td>1418644652</td>\n",
       "      <td>rkelly1972a</td>\n",
       "      <td>Robert Kelly</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other than Satan, is there anyone more evil th...</td>\n",
       "      <td>['seanhannity', 'speakerpelosi']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['https://pbs.twimg.com/media/ET-euAMWsAEZ7nP....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['#stimulusplan', '#covid2019', '#swampcreatur...</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/rkelly1972a/status/1242886...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'user_id': '1418644652', 'username': 'rkelly...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id      conversation_id     created_at        date  \\\n",
       "0  1242886541418037251  1241512518721376257  1585162217000  2020-03-25   \n",
       "1  1242886541271273473  1242886541271273473  1585162217000  2020-03-25   \n",
       "2  1242886540839305216  1242886540839305216  1585162217000  2020-03-25   \n",
       "3  1242886539786350592  1242886539786350592  1585162217000  2020-03-25   \n",
       "4  1242886539564257292  1242834296358014977  1585162217000  2020-03-25   \n",
       "\n",
       "       time timezone              user_id       username  \\\n",
       "0  14:50:17      EDT           2758562689     adrisml777   \n",
       "1  14:50:17      EDT  1212444154761863168    sammendment   \n",
       "2  14:50:17      EDT   939935680137920515   still_a_nerd   \n",
       "3  14:50:17      EDT            144702285  sebasaherrera   \n",
       "4  14:50:17      EDT           1418644652    rkelly1972a   \n",
       "\n",
       "                    name place  \\\n",
       "0     Adrian Finn George   NaN   \n",
       "1       Second Amendment   NaN   \n",
       "2  the coronavirus movie   NaN   \n",
       "3      Sebastian Herrera   NaN   \n",
       "4           Robert Kelly   NaN   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  I said they are better than nothing, not that ...   \n",
       "1  Oops: CNN Forgot To Translate Its Latest Coron...   \n",
       "2  Speeding up https://twitter.com/ryanfiredragon...   \n",
       "3  Texas Gov. Greg Abbott, who routinely overrule...   \n",
       "4  Other than Satan, is there anyone more evil th...   \n",
       "\n",
       "                           mentions  \\\n",
       "0       ['mmmbiata', 'iatemuggles']   \n",
       "1                                []   \n",
       "2                                []   \n",
       "3               ['efindell', 'wsj']   \n",
       "4  ['seanhannity', 'speakerpelosi']   \n",
       "\n",
       "                                                urls  \\\n",
       "0                                                 []   \n",
       "1  ['https://babylonbee.com/news/cnn-embarassed-a...   \n",
       "2  ['https://twitter.com/ryanfiredragon/status/12...   \n",
       "3  ['https://www.wsj.com/articles/texas-coronavir...   \n",
       "4                                                 []   \n",
       "\n",
       "                                              photos  replies_count  \\\n",
       "0                                                 []              0   \n",
       "1                                                 []              0   \n",
       "2                                                 []              0   \n",
       "3                                                 []              0   \n",
       "4  ['https://pbs.twimg.com/media/ET-euAMWsAEZ7nP....              0   \n",
       "\n",
       "   retweets_count  likes_count  \\\n",
       "0               0            0   \n",
       "1               0            0   \n",
       "2               0            0   \n",
       "3               0            0   \n",
       "4               0            0   \n",
       "\n",
       "                                            hashtags cashtags  \\\n",
       "0                                                 []       []   \n",
       "1                                                 []       []   \n",
       "2                                                 []       []   \n",
       "3                                                 []       []   \n",
       "4  ['#stimulusplan', '#covid2019', '#swampcreatur...       []   \n",
       "\n",
       "                                                link  retweet  \\\n",
       "0  https://twitter.com/AdrisML777/status/12428865...    False   \n",
       "1  https://twitter.com/SAmmendment/status/1242886...    False   \n",
       "2  https://twitter.com/still_a_nerd/status/124288...    False   \n",
       "3  https://twitter.com/SebasAHerrera/status/12428...    False   \n",
       "4  https://twitter.com/rkelly1972a/status/1242886...    False   \n",
       "\n",
       "                                           quote_url  video  near  geo  \\\n",
       "0                                                NaN      0   NaN  NaN   \n",
       "1                                                NaN      0   NaN  NaN   \n",
       "2  https://twitter.com/ryanfiredragon/status/1242...      0   NaN  NaN   \n",
       "3                                                NaN      0   NaN  NaN   \n",
       "4                                                NaN      0   NaN  NaN   \n",
       "\n",
       "   source  user_rt_id  user_rt  retweet_id  \\\n",
       "0     NaN         NaN      NaN         NaN   \n",
       "1     NaN         NaN      NaN         NaN   \n",
       "2     NaN         NaN      NaN         NaN   \n",
       "3     NaN         NaN      NaN         NaN   \n",
       "4     NaN         NaN      NaN         NaN   \n",
       "\n",
       "                                            reply_to  retweet_date  translate  \\\n",
       "0  [{'user_id': '2758562689', 'username': 'AdrisM...           NaN        NaN   \n",
       "1  [{'user_id': '1212444154761863168', 'username'...           NaN        NaN   \n",
       "2  [{'user_id': '939935680137920515', 'username':...           NaN        NaN   \n",
       "3  [{'user_id': '144702285', 'username': 'SebasAH...           NaN        NaN   \n",
       "4  [{'user_id': '1418644652', 'username': 'rkelly...           NaN        NaN   \n",
       "\n",
       "   trans_src  trans_dest  \n",
       "0        NaN         NaN  \n",
       "1        NaN         NaN  \n",
       "2        NaN         NaN  \n",
       "3        NaN         NaN  \n",
       "4        NaN         NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preliminary look at our data\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85716"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop unnecessary columns\n",
    "data = pd.DataFrame(df['tweet'])\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a VADER sentiment analyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset = ['tweet'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_string = []\n",
    "for i in df.tweet:\n",
    "    if type(i) is not str:\n",
    "        non_string.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85715"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentiment scores, store compound sentiment score as our sentiment column\n",
    "data['sentiment'] = [analyser.polarity_scores(i) for i in data['tweet']]\n",
    "data['compound_sentiment'] = [i['compound'] for i in df['sentiment']]\n",
    "data['sentiment'] = df['compound_sentiment']\n",
    "data.drop(columns=['compound_sentiment'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I said they are better than nothing, not that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oops: CNN Forgot To Translate Its Latest Coron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Speeding up https://twitter.com/ryanfiredragon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Texas Gov. Greg Abbott, who routinely overrule...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Other than Satan, is there anyone more evil th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0  I said they are better than nothing, not that ...\n",
       "1  Oops: CNN Forgot To Translate Its Latest Coron...\n",
       "2  Speeding up https://twitter.com/ryanfiredragon...\n",
       "3  Texas Gov. Greg Abbott, who routinely overrule...\n",
       "4  Other than Satan, is there anyone more evil th..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view sentiment distribution\n",
    "df.sentiment.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize tweets\n",
    "data['tweet_vec'] = data['tweet'].map(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I said they are better than nothing, not that ...</td>\n",
       "      <td>[I, said, they, are, better, than, nothing, ,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oops: CNN Forgot To Translate Its Latest Coron...</td>\n",
       "      <td>[Oops, :, CNN, Forgot, To, Translate, Its, Lat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Speeding up https://twitter.com/ryanfiredragon...</td>\n",
       "      <td>[Speeding, up, https, :, //twitter.com/ryanfir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Texas Gov. Greg Abbott, who routinely overrule...</td>\n",
       "      <td>[Texas, Gov, ., Greg, Abbott, ,, who, routinel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Other than Satan, is there anyone more evil th...</td>\n",
       "      <td>[Other, than, Satan, ,, is, there, anyone, mor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  I said they are better than nothing, not that ...   \n",
       "1  Oops: CNN Forgot To Translate Its Latest Coron...   \n",
       "2  Speeding up https://twitter.com/ryanfiredragon...   \n",
       "3  Texas Gov. Greg Abbott, who routinely overrule...   \n",
       "4  Other than Satan, is there anyone more evil th...   \n",
       "\n",
       "                                           tweet_vec  \n",
       "0  [I, said, they, are, better, than, nothing, ,,...  \n",
       "1  [Oops, :, CNN, Forgot, To, Translate, Its, Lat...  \n",
       "2  [Speeding, up, https, :, //twitter.com/ryanfir...  \n",
       "3  [Texas, Gov, ., Greg, Abbott, ,, who, routinel...  \n",
       "4  [Other, than, Satan, ,, is, there, anyone, mor...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [I, said, they, are, better, than, nothing, ,,...\n",
       "1        [Oops, :, CNN, Forgot, To, Translate, Its, Lat...\n",
       "2        [Speeding, up, https, :, //twitter.com/ryanfir...\n",
       "3        [Texas, Gov, ., Greg, Abbott, ,, who, routinel...\n",
       "4        [Other, than, Satan, ,, is, there, anyone, mor...\n",
       "                               ...                        \n",
       "13714    [RT, to, tell, ⁦, @, USAID⁩, to, stop, aid, cu...\n",
       "13715    [For, those, of, you, who, 've, been, followin...\n",
       "13716    [Maybe, ,, his, death, will, save, the, lives,...\n",
       "13717    [y, ’, all, go, watch, this, corona, virus, mo...\n",
       "13718    [@, USDOL, just, released, this, to, workplace...\n",
       "Name: tweet_vec, Length: 85715, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data['tweet_vec']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate a w2v model\n",
    "w2v_model = gensim.models.Word2Vec(data, size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189612895, 247008700)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the w2v model\n",
    "w2v_model.train(data, total_examples=w2v_model.corpus_count, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get what matters (weights, etc..), throw away the actual model to reclaim memory space\n",
    "word_vectors = w2v_model.wv\n",
    "del w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('President', 0.6168984174728394),\n",
       " ('administration', 0.5924678444862366),\n",
       " ('government', 0.5718132257461548),\n",
       " ('he', 0.5284136533737183),\n",
       " ('realDonaldTrump', 0.5188762545585632),\n",
       " ('presidency', 0.5158246755599976),\n",
       " ('leader', 0.5008671283721924),\n",
       " ('ego', 0.49674510955810547),\n",
       " ('bigot', 0.4958049952983856),\n",
       " ('UAZMedTucson', 0.48750990629196167)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['president'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_vectors.accuracy('questions-words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join(df['SentimentText'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = all_words.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_dict = {}\n",
    "for i in all_words:\n",
    "    if i not in all_words_dict.keys():\n",
    "        all_words_dict[i] = 1\n",
    "    else:\n",
    "        all_words_dict[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_words = sorted(all_words_dict, key=all_words_dict.get, reverse=True)\n",
    "sorted_words.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(set(nltk.corpus.stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_stop = ['and',\n",
    "             'of',\n",
    "             'in',\n",
    "             'he',\n",
    "             'was',\n",
    "             'at',\n",
    "             'not',\n",
    "             'as',\n",
    "             'it',\n",
    "             'I',\n",
    "             'but',\n",
    "             'she',\n",
    "             'The',\n",
    "             'said',\n",
    "             'were',\n",
    "             'be',\n",
    "             'which',\n",
    "             'who',\n",
    "             'one',\n",
    "             'He',\n",
    "             'an',\n",
    "             'he',\n",
    "             'as',\n",
    "             'she',\n",
    "             'be',\n",
    "             'He',\n",
    "             'Planet',\n",
    "             'eBooks',\n",
    "             'Free',\n",
    "             'eBook.com',\n",
    "             'could',\n",
    "             'no',\n",
    "             'will',\n",
    "             'now',\n",
    "             'went',\n",
    "             'But',\n",
    "             'And',\n",
    "             'him.',\n",
    "             'man',\n",
    "             '‘I',\n",
    "             'old',\n",
    "             'him,',\n",
    "             'She',\n",
    "             'would',\n",
    "             'eBook.com',\n",
    "             'him.',\n",
    "             'him,',\n",
    "             'like',\n",
    "             'came',\n",
    "             'looked',\n",
    "             'time',\n",
    "             'began',\n",
    "             'know',\n",
    "             'without',\n",
    "             'face',\n",
    "             'thought',\n",
    "             'felt',\n",
    "             'see',\n",
    "             'still',\n",
    "             'ing',\n",
    "             'men',\n",
    "             'It',\n",
    "             'seemed',\n",
    "             'left',\n",
    "             'go',\n",
    "             'little',\n",
    "             'come',\n",
    "             'In',\n",
    "             'eyes',\n",
    "             'two',\n",
    "             'even',\n",
    "             'must',\n",
    "             'long',\n",
    "             'saw',\n",
    "             'made',\n",
    "             'something',\n",
    "             'go',\n",
    "             'come',\n",
    "             'always',\n",
    "             'it.',\n",
    "             'heard',\n",
    "             'say',\n",
    "             'army',\n",
    "             'and,',\n",
    "             'young',\n",
    "             'took',\n",
    "             'told',\n",
    "             'don’t',\n",
    "             'back',\n",
    "             'the',\n",
    "             'a',\n",
    "             'for',\n",
    "             'of',\n",
    "             'have',\n",
    "             'me',\n",
    "             'was',\n",
    "             \"I'm\",\n",
    "             'are',\n",
    "             'with',\n",
    "             '-',\n",
    "             '_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stop_words + more_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more = [\"news\",\"case\",\"make\",\"would\",\"death\",\"spread\",\"story\",\"still\",\"stop\",\"kill\",\"com\", \"twitter\",\"pic\",\"say\",\"https\"]\n",
    "more2 = [\"help\",\"work\",\"time\",\"new\",\"covid\",\"get\",\"test\",\"day\",\"http\",\"country\",\"good\",\"world\",\"may\",\"government\",\"pat\"]\n",
    "more3 = [\"due\",\"stay\",\"be\",\"report\",\"worker\",\"bill\",\"back\",\"first\",\"people\",\"corona\"]\n",
    "more4 = [\"go\",\"die\",\"take\",\"see\",\"know\",\"status\",\"call\",'Go','Say', 'www', 'many', 'see', 'make', 'want', 'be', 'could', \"s\", 'tell', 'today']\n",
    "more5 = ['Let', 'Be', 'case', 'make', 'also', 'let']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more.extend(more2)\n",
    "more.extend(more3)\n",
    "more.extend(more4)\n",
    "more.extend(more5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "stop_words.extend(more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['SentimentText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = []\n",
    "for i in data:\n",
    "    types.append(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "non_str = {}\n",
    "fake = []\n",
    "for i in data: \n",
    "    if type(i) != str:\n",
    "        non_str[index] = type(i)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in non_str:\n",
    "    data = np.delete(data, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out tweerts with fewer than 10 words\n",
    "ndata = []\n",
    "for i in data:\n",
    "    if len(i.split(' ')) > 10:\n",
    "        ndata.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data), len(ndata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a text pre-processing pipeline\n",
    "def corpus_pipeline(data):  \n",
    "    # ********    THE FOLLOWING REGEXES SHOULD PROBABLY BE HEAVILY MODIFIED FOR CUSTOMIZING TWEETS CLEANING  *************\n",
    "    # Remove Emails\n",
    "    data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "    def sent_to_words(sentences):\n",
    "        for sentence in sentences:\n",
    "            yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "    data_words = list(sent_to_words(data))\n",
    "\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "    # # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    # **********   ALSO PROBABLY REQUIRES EXTENSIVE ADDING OF TWEET-SPECIFIC STOPWORDS   ***********\n",
    "    # Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "    def remove_stopwords(texts):\n",
    "        return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "    def make_bigrams(texts):\n",
    "        return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "    def make_trigrams(texts):\n",
    "        return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "    def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "        texts_out = []\n",
    "        for sent in texts:\n",
    "            doc = nlp(\" \".join(sent)) \n",
    "            texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        return texts_out\n",
    "\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "    \n",
    "    data_words_trigrams = make_trigrams(data_words_bigrams)\n",
    "\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized = lemmatization(data_words_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = data_lemmatized\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    \n",
    "    return id2word, texts, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store results from our pipeline function call\n",
    "id2word, texts, corpus = corpus_pipeline(ndata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute coherence values for various number of topics.\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=5, step=5): \n",
    "    \"\"\" Compute coherence values for various number of topics.\n",
    "    Parameters:\n",
    "        dictionary : Gensim dictionary\n",
    "        corpus : Gensim corpus\n",
    "        texts : List of input texts\n",
    "        limit : Max num of topics\n",
    "        start : Int of lowest number of topics to model\n",
    "        step : Int of increment to change number of topics to model\n",
    "    Returns:\n",
    "        model_list : List of LDA topic models\n",
    "        coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        start=time.time()\n",
    "        model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=1000,\n",
    "                                           passes=1,\n",
    "                                           per_word_topics=True)\n",
    "                                            \n",
    "        \n",
    "        print(f'Topic modeling for {num_topics} topics took {time.time()-start} seconds.')\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strt = 12\n",
    "lmt = 17\n",
    "stp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store computed coherence values\n",
    "ml, cov = compute_coherence_values(id2word, corpus, texts, lmt, strt, stp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_coherence(coherence_values):\n",
    "    \"\"\" Graph a list of coherence values to determine the optimal number of topics to model. \"\"\"\n",
    "    limit=lmt; start=strt; step=stp;\n",
    "    x = range(start, limit, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_coherence(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model = ml[-1]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(optimal_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model_5 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_5.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model_10 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_10.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save a dictionary into a pickle file.\n",
    "#import pickle\n",
    "#\n",
    "#pickle.dump(lda_model_10, open(\"mode1_10_345\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#lda_model_10 = pickle.load( open( \"mode1_10_345\", \"rb\" ) )\n",
    "## favorite_color is now { \"lion\": \"yellow\", \"kitty\": \"red\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model = lda_model_10\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_10, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_5, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r\"[a-zA-Z+]+\"\n",
    "\n",
    "topic_1_str = '0.067*\\\"thank\\\" + 0.030*\\\"feel\\\" + 0.030*\\\"look\\\" + 0.026*\\\"tweet\\\" + 0.025*\\\"good\\\" + 0.023*\\\"watch\\\" + 0.017*\\\"keep\\\" + 0.016*\\\"sleep\\\" + 0.014*\\\"make\\\" + 0.012*\\\"see\\\"'\n",
    "topic_2_str = '0.068*\"s\" + 0.021*\"fun\" + 0.021*\"wish\" + 0.020*\"find\" + 0.016*\"lot\" + 0.015*\"tomorrow\" + 0.015*\"morning\" + 0.014*\"give\" + 0.014*\"year\" + 0.013*\"weekend\"'\n",
    "topic_3_str = '0.077*\"be\" + 0.076*\"not\" + 0.039*\"do\" + 0.023*\"get\" + 0.022*\"go\" + 0.021*\"love\" + 0.018*\"have\" + 0.018*\"can\" + 0.017*\"think\" + 0.014*\"really\"'\n",
    "topic_4_str = '0.058*\"quot\" + 0.035*\"need\" + 0.035*\"lol\" + 0.028*\"try\" + 0.026*\"bad\" + 0.017*\"sound\" + 0.012*\"check\" + 0.011*\"enjoy\" + 0.011*\"hate\" + 0.011*\"day\"'\n",
    "topic_5_str = '0.034*\"hope\" + 0.031*\"amp\" + 0.026*\"great\" + 0.020*\"last\" + 0.018*\"week\" + 0.017*\"sorry\" + 0.016*\"night\" + 0.015*\"follow\" + 0.014*\"mean\" + 0.014*\"start\"'\n",
    "\n",
    "matches_1 = re.finditer(regex, topic_1_str)\n",
    "matches_2 = re.finditer(regex, topic_2_str)\n",
    "matches_3 = re.finditer(regex, topic_3_str)\n",
    "matches_4 = re.finditer(regex, topic_4_str)\n",
    "matches_5 = re.finditer(regex, topic_5_str)\n",
    "\n",
    "matches_list = [matches_1, matches_2, matches_3, matches_4, matches_5]\n",
    "\n",
    "topic_dict = {}\n",
    "count = 0\n",
    "for i in matches_list:\n",
    "    count += 1\n",
    "    topic_list = []\n",
    "    for j, match in enumerate(i, start=1):\n",
    "        if j%2 != 0:\n",
    "            topic_list.append(match.group())\n",
    "    topic_dict.setdefault('topic_{}'.format(count), topic_list)\n",
    "topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec-Keras DIY Sentimentizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sorted_words:\n",
    "    if i in stop_words:\n",
    "        sorted_words.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_words_dict = {}\n",
    "for i in sorted_words:\n",
    "    sorted_words_dict[i] = all_words_dict[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_list = []\n",
    "for i in sorted_words:\n",
    "    lemmatized_list.append(lemmatizer.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique words: {}'.format(len(sorted_words_dict)))\n",
    "print(len('Number of lemmatized stems: {}'.format(lemmatized_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_dict = {}\n",
    "for i in lemmatized_list:\n",
    "    if i in lemmatized_dict.keys():\n",
    "        lemmatized_dict[i] += 1\n",
    "    else:\n",
    "        lemmatized_dict[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lemmatized_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = word_vectors.get_keras_embedding(train_embeddings=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = models.Sequential()\n",
    "#\n",
    "#model.add(layers.embeddings.Embedding(input_dim=max_words,\n",
    "#  output_dim=embed_vec_len, embeddings_initializer=e_init,\n",
    "#  mask_zero=True))\n",
    "#\n",
    "## specify the first hidden layer\n",
    "#model.add(Dense(50, activation='relu', input_shape=(784,)))\n",
    "#\n",
    "## specify the second layer\n",
    "#model.add(Dense(50, activation='relu'))\n",
    "#\n",
    "## specify the output layer\n",
    "#model.add(Dense(num_classes, activation='sigmoid'))\n",
    "#\n",
    "#model.summary()\n",
    "#\n",
    "#model.compile(loss='categorical_crossentropy',\n",
    "#              optimizer=SGD(),\n",
    "#              metrics=['accuracy'])\n",
    "#\n",
    "#history = model.fit(x_train, y_train,\n",
    "#                    batch_size=batch_size,\n",
    "#                    epochs=epochs,\n",
    "#                    verbose=1,\n",
    "#                    validation_data=(x_test, y_test))\n",
    "#score = model.evaluate(x_test, y_test, verbose=0)\n",
    "#print('Test loss:', score[0])\n",
    "#print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "335.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
